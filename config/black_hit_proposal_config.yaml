# Test config file for Blip
module:
  module_name:  'blank_config'
  module_type:  ['ml']
  module_mode:  ['training']
  gpu:          True
  gpu_device:   0
  verbose:      False

dataset:
  # ---- Arrakis ----
  # This section defines the 'arrakis.py' submodule which takes in
  # outputs from different Arrakis iterations and constructs a common dataset format
  # in .npz or .hdf5 files.
  # If this is the first time processing the simulation output
  process_simulation: False
  simulation_type:    'LArSoft'
  simulation_folder:  ""
  simulation_files:   []

  # ---- Blip ----
  dataset_type:   ""
  dataset_folder: ""
  dataset_files:  []
  normalized:     False
  root:           "."
  skip_processing:  False
  transform:        null
  pre_transform:    null
  pre_filter:       null
  # ---- variables ----
  # The following variables are enough to describe a general set of tasks.  In tasks involving
  # semantic segmentation or point clouds networks, there should be a distinction between 
  # 'positions' and 'features', the former of which defines the grid/point cloud geometry,
  # while the later refers to discriminating variables.
  positions:      []
  features:       []
  classes:        []
  labels:         []
  clusters:       []
  hits:           []
  # ---- normalization ----
  # Normalization parameters for positions and features.  Typically we want to normalize
  # input features depending on the type of task, or to unbias certain simulation parameters,
  # e.g. by normalizaing ADC over an event sample we remove the overall scale of the detector
  # response.
  position_normalization:   []
  features_normalization:   []
  # ---- weights ----
  # Class and sample weights are used for imbalanced datasets.
  class_weights:  []
  sample_weights: []
  # ---- clustering ----
  # Clustering parameters are used for creating cluster datasets, i.e. datasets where each
  # sample is a cluster, rather than an entire event. The current clustering method is DBSCAN, 
  # but this will likely change in the future to accomidate other approaches.  If cluster_class
  # is specified, then only types 'cluster_class:cluster_label' will be turned into cluster
  # datasets.  The 'cluster_variables' are the variables to be considered in the clustering.
  dbscan_min_samples: 1
  dbscan_eps:         0.1
  cluster_class:      ""
  cluster_label:      ""
  cluster_variables:  []
  # ---- masks ----
  # Masks are used to pick out points/events which
  # are only of type 'class_mask:label_mask'.
  class_mask:   ""
  label_mask:   ""
  # we can regroup classes to reduce their number,
  # e.g., say we want to train so that there are
  # only two important shapes, "blip" and "not-blip",
  # then we would consolidate like below:
  consolidate_classes:  

loader:
  loader_type:      ""
  batch_size:       1
  test_split:       0.0
  test_seed:        0
  validation_split: 0.0
  validation_seed:  0
  num_workers:      1

training:
  epochs:       1
  checkpoint:   1
  progress_bar: ""        # train, validation, test, all
  rewrite_bar:      False # wether to leave bars after each epoch
  save_predictions: True  # wether to save network outputs in original file
  no_timing:    False     # wether to keep timing/memory info in callback
  seed:         0

model:
  # uncomment the line below and specify the model to load from a checkpoint.
  # load_model:   ".checkpoints/checkpoint_200.ckpt"

  # multiple options for model_type: 
  # [ "single", "composite", ... ]
  model_type:       "single"
  HitProposal:
    # sparseuresnet
    in_channels:      1
    classifications:  ["topology", "particle", "physics"]               # {"source", "shape", "particle"}
    out_channels:     []                     # {8, 7, 32}
    filtrations:      [64, 128, 256, 512]     # the number of filters in each downsample
    double_conv_params:
      kernel_size:  3
      stride:       1
      dilation:     1
      activation:   "relu"
      dimension:    2
      batch_norm:   True
    conv_transpose_params:
      kernel_size:  2
      stride:       2
      dilation:     1
      dimension:    2
    max_pooling_params:
      kernel_size:  2
      stride:       2
      dilation:     1
      dimension:    2
    # point proposal
    shared_convolutions:  [256, 512]
    shared_kernel_size:   3
    shared_stride:        1
    shared_dilation:      1
    shared_activation:    'relu'
    shared_batch_norm:    true
    classification_anchors:     10
    classification_size:        1
    classification_kernel_size: 1
    regression_anchors:     10
    regression_size:        7
    regression_kernel_size: 1

criterion:
    FocalLoss:
      alpha:  1.0
      
metrics:

callbacks:

optimizer:
  optimizer_type: "Adam"
  learning_rate:  0.01
  betas:          [0.9, 0.999]
  epsilon:        1e-08
  weight_decay:   0.001
  momentum:       0.9